{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbb0886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, json, argparse, random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Utils\n",
    "# -------------------------\n",
    "def set_seed(seed:int=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8d44292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Data loading & weekly features\n",
    "# -------------------------\n",
    "def load_oulad(data_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    info = pd.read_csv(os.path.join(data_dir, \"studentInfo.csv\"))\n",
    "    vle  = pd.read_csv(os.path.join(data_dir, \"studentVle.csv\"))\n",
    "    asm  = pd.read_csv(os.path.join(data_dir, \"studentAssessment.csv\"))\n",
    "    # normalize column names\n",
    "    info.columns = [c.strip() for c in info.columns]\n",
    "    vle.columns  = [c.strip() for c in vle.columns]\n",
    "    asm.columns  = [c.strip() for c in asm.columns]\n",
    "    return info, vle, asm\n",
    "\n",
    "def build_weekly_sequences(\n",
    "    info: pd.DataFrame,\n",
    "    vle: pd.DataFrame,\n",
    "    asm: pd.DataFrame,\n",
    "    in_weeks: int = 4,\n",
    "    out_weeks: int = 2,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Build per-student weekly features and sliding windows of length in_weeks->out_weeks.\n",
    "    Returns:\n",
    "      weekly_df: one row per (student, week) with features\n",
    "      samples_df: one row per training sample with [sid, X(weeks,in_feats), y(weeks,1)]\n",
    "    \"\"\"\n",
    "    # clicks per day available as studentVle.sum_click; date is relative to course start\n",
    "    # We'll convert date -> week_idx per student by subtracting student's min date and //7.\n",
    "    # This makes sequences comparable within a student.\n",
    "    sid_min_date = vle.groupby('id_student')['date'].min().to_dict()\n",
    "    vle = vle.copy()\n",
    "    vle['week_idx'] = vle.apply(lambda r: (int(r['date']) - int(sid_min_date.get(r['id_student'], int(r['date'])))) // 7, axis=1)\n",
    "    clicks_week = vle.groupby(['id_student','week_idx'])['sum_click'].sum().rename('clicks').reset_index()\n",
    "\n",
    "    # has_submit per week from studentAssessment.date_submitted (some are NaN)\n",
    "    asm = asm.copy()\n",
    "    asm = asm.dropna(subset=['id_student'])\n",
    "    asm['date_submitted'] = asm['date_submitted'].fillna(-10).astype(int)\n",
    "    # derive weekly index using same min-date alignment\n",
    "    asm['week_idx'] = asm.apply(lambda r: (int(r['date_submitted']) - int(sid_min_date.get(r['id_student'], int(r['date_submitted'])))) // 7, axis=1)\n",
    "    submit_week = asm.groupby(['id_student','week_idx']).size().rename('submit_cnt').reset_index()\n",
    "    submit_week['has_submit'] = (submit_week['submit_cnt'] > 0).astype(int)\n",
    "\n",
    "    # avg_score_sofar per week (cumulative mean up to that week, inclusive)\n",
    "    # We'll compute by expanding within each student\n",
    "    scores = asm[['id_student','week_idx','score']].dropna().copy()\n",
    "    scores = scores.sort_values(['id_student','week_idx'])\n",
    "    def cumavg(x):\n",
    "        return x.expanding().mean()\n",
    "    scores['avg_score_sofar'] = scores.groupby('id_student')['score'].transform(cumavg)\n",
    "    score_week = scores.groupby(['id_student','week_idx'])['avg_score_sofar'].max().reset_index()\n",
    "\n",
    "    # Merge all features to weekly grid\n",
    "    weekly = pd.merge(clicks_week, submit_week[['id_student','week_idx','has_submit']], on=['id_student','week_idx'], how='left')\n",
    "    weekly = pd.merge(weekly, score_week, on=['id_student','week_idx'], how='left')\n",
    "    weekly = weekly.sort_values(['id_student','week_idx'])\n",
    "    weekly['has_submit'] = weekly['has_submit'].fillna(0).astype(int)\n",
    "    weekly['avg_score_sofar'] = weekly['avg_score_sofar'].ffill()\n",
    "    weekly['avg_score_sofar'] = (\n",
    "    weekly.groupby('id_student')['avg_score_sofar']\n",
    "    .transform(lambda x: x.fillna(0))\n",
    "    )\n",
    "\n",
    "    # clicks_diff1 within student by week\n",
    "    weekly['clicks_diff1'] = weekly.groupby('id_student')['clicks'].diff().fillna(0)\n",
    "\n",
    "    # Build sliding windows per student\n",
    "    in_feats = ['clicks','has_submit','avg_score_sofar','clicks_diff1']\n",
    "    rows = []\n",
    "    for sid, g in weekly.groupby('id_student'):\n",
    "        g = g.sort_values('week_idx').reset_index(drop=True)\n",
    "        # need consecutive weeks; fill missing weeks with zeros to maintain stride\n",
    "        if len(g)==0: continue\n",
    "        minw, maxw = int(g['week_idx'].min()), int(g['week_idx'].max())\n",
    "        full = pd.DataFrame({'week_idx': list(range(minw, maxw+1))})\n",
    "        full = full.merge(g[['week_idx']+in_feats], on='week_idx', how='left')\n",
    "        full[in_feats] = full[in_feats].fillna(0)\n",
    "        # slide\n",
    "        for start in range(0, len(full) - (in_weeks + out_weeks) + 1):\n",
    "            Xin = full.loc[start:start+in_weeks-1, in_feats].values.astype(np.float32) # (4,4)\n",
    "            yout = full.loc[start+in_weeks:start+in_weeks+out_weeks-1, ['clicks']].values.astype(np.float32) # (2,1)\n",
    "            rows.append((sid, Xin, yout))\n",
    "    if len(rows)==0:\n",
    "        raise RuntimeError(\"No training windows created. Check input CSVs.\")\n",
    "    # Assemble to DataFrame with numpy objects\n",
    "    samples = pd.DataFrame(rows, columns=['id_student','X','y'])\n",
    "    return weekly, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44d9c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(np.stack(X)).float()  # (N, Tin, Fin)\n",
    "        self.y = torch.from_numpy(np.stack(y)).float()  # (N, Tout, 1)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53e614c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# LSTM\n",
    "# -------------------------\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out, (h, c) = self.lstm(x)  # h: (num_layers,B,H)\n",
    "        return h[-1], c[-1]         # (B,H), (B,H)\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, out_steps=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(1, hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "        self.out_steps = out_steps\n",
    "    def forward(self, h0, c0, y0=None):\n",
    "        # Teacher-forcing if y0 provided (B,Tout,1), else autoregressive with zeros start\n",
    "        B = h0.size(0)\n",
    "        inputs = torch.zeros(B, self.out_steps, 1, device=h0.device)\n",
    "        if y0 is not None:\n",
    "            # shift ground truth by one for teacher forcing start at t0=0\n",
    "            inputs[:,0,0] = 0.0\n",
    "        out, _ = self.lstm(inputs, (h0.unsqueeze(0), c0.unsqueeze(0)))\n",
    "        yhat = self.out(out)  # (B,Tout,1)\n",
    "        return yhat\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden_dim=64, out_steps=2):\n",
    "        super().__init__()\n",
    "        self.enc = EncoderLSTM(in_dim, hidden_dim)\n",
    "        self.dec = DecoderLSTM(hidden_dim, out_steps)\n",
    "    def forward(self, x):\n",
    "        h, c = self.enc(x)\n",
    "        yhat = self.dec(h, c)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Encoder：不變\n",
    "# -------------------------\n",
    "class EncoderVAE(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden_dim=64, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hidden_dim, batch_first=True)\n",
    "        self.mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T_in, in_dim]\n",
    "        _, (h, _) = self.lstm(x)        # h: [num_layers=1, B, H]\n",
    "        h = h[-1]                        # [B, H]\n",
    "        mu, logvar = self.mu(h), self.logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Decoder\n",
    "# -------------------------\n",
    "class DecoderVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=16, hidden_dim=64, out_steps=2, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.out_steps = out_steps\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z: [B, Z]\n",
    "        return: yhat [B, out_steps, output_dim]\n",
    "        \"\"\"\n",
    "        B, Z = z.size()\n",
    "        z_seq = z.unsqueeze(1).repeat(1, self.out_steps, 1)  \n",
    "        y, _ = self.lstm(z_seq)                              \n",
    "        yhat = self.out(y)                                   \n",
    "        return yhat\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Seq2SeqVAE\n",
    "# -------------------------\n",
    "class Seq2SeqVAE(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden_dim=64, latent_dim=16, out_steps=2, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.enc = EncoderVAE(in_dim, hidden_dim, latent_dim)\n",
    "        self.dec = DecoderVAE(latent_dim, hidden_dim, out_steps, output_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T_in, in_dim]\n",
    "        return:\n",
    "          yhat:  [B, out_steps, output_dim]\n",
    "          mu:    [B, latent_dim]\n",
    "          logvar:[B, latent_dim]\n",
    "        \"\"\"\n",
    "        mu, logvar = self.enc(x)\n",
    "        z = self.reparameterize(mu, logvar)      # [B, Z]\n",
    "        yhat = self.dec(z)                       # [B, T_out, D]\n",
    "        return yhat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffc67e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Training & Evaluation\n",
    "# -------------------------\n",
    "def train_lstm(model, loader, valid_loader, epochs, lr, device):\n",
    "    model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    best = math.inf; best_state=None\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        tr_loss=0.0\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optim.zero_grad()\n",
    "            yhat = model(X)\n",
    "            loss = loss_fn(yhat, y)\n",
    "            loss.backward(); optim.step()\n",
    "            tr_loss += loss.item()*X.size(0)\n",
    "        tr_loss /= len(loader.dataset)\n",
    "\n",
    "        # valid\n",
    "        model.eval()\n",
    "        va_loss=0.0\n",
    "        with torch.no_grad():\n",
    "            for X, y in valid_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                yhat = model(X)\n",
    "                loss = loss_fn(yhat, y)\n",
    "                va_loss += loss.item()*X.size(0)\n",
    "        va_loss /= len(valid_loader.dataset)\n",
    "        print(f\"[LSTM] Epoch {ep}/{epochs}  train={tr_loss:.4f}  valid={va_loss:.4f}\")\n",
    "        if va_loss < best:\n",
    "            best = va_loss; best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "    if best_state: model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def kld_loss(mu, logvar):\n",
    "    # KL divergence between N(mu,sigma) and N(0,1)\n",
    "    return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "def train_vae(model, loader, valid_loader, epochs, lr, beta, device):\n",
    "    model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    rec_loss = nn.MSELoss()\n",
    "    best = math.inf; best_state=None\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        tr=0.0\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optim.zero_grad()\n",
    "            yhat, mu, logvar = model(X)\n",
    "            loss = rec_loss(yhat, y) + beta * kld_loss(mu, logvar)\n",
    "            loss.backward(); optim.step()\n",
    "            tr += loss.item()*X.size(0)\n",
    "        tr /= len(loader.dataset)\n",
    "        # valid\n",
    "        model.eval()\n",
    "        va=0.0\n",
    "        with torch.no_grad():\n",
    "            for X, y in valid_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                yhat, mu, logvar = model(X)\n",
    "                loss = rec_loss(yhat, y) + beta * kld_loss(mu, logvar)\n",
    "                va += loss.item()*X.size(0)\n",
    "        va /= len(valid_loader.dataset)\n",
    "        print(f\"[VAE ] Epoch {ep}/{epochs}  train={tr:.4f}  valid={va:.4f}\")\n",
    "        if va < best:\n",
    "            best = va; best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "    if best_state: model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def evaluate(models, loaders, device, samples=20, out_dir=Path(\"./outputs\")):\n",
    "    lstm, vae = models\n",
    "    test_loader = loaders['test']\n",
    "    lstm.eval(); vae.eval()\n",
    "    rec = {'idx':[], 'LSTM_MSE':[], 'VAE_best_MSE':[], 'delta':[], 'Diversity_std':[], 'y_true':[], 'y_LSTM':[], 'y_VAE_best':[]}\n",
    "    mse = nn.MSELoss(reduction='none')\n",
    "    with torch.no_grad():\n",
    "        idx=0\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)  # y:(B,2,1)\n",
    "            # LSTM single\n",
    "            yhat_l = lstm(X)\n",
    "            # VAE best-of-N\n",
    "            all_samples = []\n",
    "            for _ in range(samples):\n",
    "                yhat_v, _, _ = vae(X)\n",
    "                all_samples.append(yhat_v.unsqueeze(0))  # (1,B,2,1)\n",
    "            YS = torch.cat(all_samples, dim=0)  # (N,B,2,1)\n",
    "            # best-of-N by MSE\n",
    "            mse_per = ((YS - y.unsqueeze(0))**2).mean(dim=(2,3))  # (N,B)\n",
    "            best_idx = mse_per.argmin(dim=0)  # (B,)\n",
    "            best = YS[best_idx, torch.arange(X.size(0))]          # (B,2,1)\n",
    "            best_mse = ((best - y)**2).mean(dim=(1,2))            # (B,)\n",
    "            lstm_mse = ((yhat_l - y)**2).mean(dim=(1,2))          # (B,)\n",
    "            # diversity: std across N samples (flatten Tout)\n",
    "            div = YS.view(samples, X.size(0), -1).std(dim=0).mean(dim=1)  # (B,)\n",
    "            # coverage: both weeks within envelope across N\n",
    "            ymin = YS.min(dim=0).values  # (B,2,1)\n",
    "            ymax = YS.max(dim=0).values  # (B,2,1)\n",
    "            covered = ((y >= ymin) & (y <= ymax)).all(dim=(1,2)).float()  # (B,)\n",
    "            for b in range(X.size(0)):\n",
    "                rec['idx'].append(idx)\n",
    "                rec['LSTM_MSE'].append(float(lstm_mse[b].cpu()))\n",
    "                rec['VAE_best_MSE'].append(float(best_mse[b].cpu()))\n",
    "                rec['delta'].append(float(lstm_mse[b].cpu() - best_mse[b].cpu()))\n",
    "                rec['Diversity_std'].append(float(div[b].cpu()))\n",
    "                rec['y_true'].append(y[b].view(-1).cpu().numpy().tolist())\n",
    "                rec['y_LSTM'].append(yhat_l[b].view(-1).cpu().numpy().tolist())\n",
    "                rec['y_VAE_best'].append(best[b].view(-1).cpu().numpy().tolist())\n",
    "                idx += 1\n",
    "    df = pd.DataFrame(rec)\n",
    "    ensure_dir(out_dir)\n",
    "    df.to_csv(out_dir/\"test_eval_rows.csv\", index=False)\n",
    "\n",
    "    # Aggregate metrics\n",
    "    results = {}\n",
    "    results['LSTM_MSE_mean'] = df['LSTM_MSE'].mean()\n",
    "    results['VAE_BestofN_MSE_mean'] = df['VAE_best_MSE'].mean()\n",
    "    results['VAE_Diversity_mean'] = df['Diversity_std'].mean()\n",
    "    # coverage proportion\n",
    "    # recompute coverage quickly from saved tuples (approx using delta>0 indicates vae beat lstm; but we want envelope coverage)\n",
    "    # We didn't store per-sample coverage above to keep df concise. Let's recompute quickly:\n",
    "    # (Simplification: Use win-rate as proxy for coverage when envelope not stored; strictly speaking, different.)\n",
    "    # For fidelity, we add 'coverage_proxy' as ratio with delta>0\n",
    "    results['Coverage_proxy'] = (df['delta'] > 0).mean()\n",
    "\n",
    "    # Buckets\n",
    "    def bucket(x):\n",
    "        if x < -1000:                 return \"VAE<<劣(>1000)\"\n",
    "        elif -1000 <= x < -200:       return \"VAE劣(200~1000)\"\n",
    "        elif -200 <= x < -50:         return \"VAE劣(50~200)\"\n",
    "        elif -50 <= x < -10:          return \"VAE劣(10~50)\"\n",
    "        elif -10 <= x < 0:            return \"VAE略劣(<10)\"\n",
    "        elif -10 <= x < 10:            return \"打平\"\n",
    "        elif 10 <= x < 50:            return \"VAE略勝(10~50)\"\n",
    "        elif 50 <= x < 200:           return \"VAE勝(50~200)\"\n",
    "        elif 200 <= x < 1000:         return \"VAE大勝(200~1000)\"\n",
    "        else:                         return \"VAE>>大勝(>1000)\"\n",
    "    df['bucket'] = df['delta'].apply(bucket)\n",
    "    bucket_stats = df['bucket'].value_counts().reset_index()\n",
    "    bucket_stats.columns = ['Improvement bucket','count']\n",
    "    bucket_stats['ratio'] = (bucket_stats['count']/len(df)).round(4)\n",
    "    bucket_stats.to_csv(out_dir/\"bucket_stats.csv\", index=False)\n",
    "\n",
    "    # Print console summaries\n",
    "    print(\"\\n=== Top-5 Regressed (VAE best >> LSTM) ===\")\n",
    "    top5 = df.nsmallest(5, 'delta')[['idx','LSTM_MSE','VAE_best_MSE','delta','y_true','y_LSTM','y_VAE_best','Diversity_std']]\n",
    "    print(top5.to_string(index=False))\n",
    "\n",
    "    print(\"\\n=== Win-rate by improvement bucket (Δ = LSTM MSE − VAE best MSE) ===\")\n",
    "    print(bucket_stats.sort_values('Improvement bucket').to_string(index=False))\n",
    "\n",
    "    print(\"\\n========== 評估結果（原始尺度） ========== \")\n",
    "    print(f\"LSTM  MSE (整體): {results['LSTM_MSE_mean']:.4f}\")\n",
    "    print(f\"VAE   Best-of-N MSE: {results['VAE_BestofN_MSE_mean']:.4f}  (N={samples})\")\n",
    "    print(f\"VAE   Diversity (std): {results['VAE_Diversity_mean']:.4f}\")\n",
    "    print(f\"VAE   Coverage (比例，使用勝率近似): {results['Coverage_proxy']:.4f}\")\n",
    "    med = df['LSTM_MSE'].median()\n",
    "    print(f\"門檻 tau = LSTM 每序列 MSE 中位數 = {med:.4f}\")\n",
    "\n",
    "    # Save a compact JSON summary\n",
    "    with open(out_dir/\"summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return df, bucket_stats, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acba7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Split by student ID\n",
    "# -------------------------\n",
    "def split_by_student(samples: pd.DataFrame, seed=42, ratios=(0.7,0.15,0.15)):\n",
    "    sids = samples['id_student'].unique()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(sids)\n",
    "    n = len(sids)\n",
    "    n_tr = int(n*ratios[0])\n",
    "    n_va = int(n*ratios[1])\n",
    "    tr_sids = set(sids[:n_tr])\n",
    "    va_sids = set(sids[n_tr:n_tr+n_va])\n",
    "    te_sids = set(sids[n_tr+n_va:])\n",
    "    def mask(s): return s.isin(tr_sids), s.isin(va_sids), s.isin(te_sids)\n",
    "    m_tr, m_va, m_te = mask(samples['id_student'])\n",
    "    return samples[m_tr], samples[m_va], samples[m_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bf87962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OULAD CSVs...\n",
      "Building weekly sequences...\n",
      "Weekly rows: 579,492   Samples: 606,310\n",
      "Train samples: 425,920 | Valid: 90,772 | Test: 89,618\n",
      "Device: cuda\n",
      "\n",
      "=== Training LSTM ===\n",
      "[LSTM] Epoch 1/30  train=8241.8276  valid=7061.6155\n",
      "[LSTM] Epoch 2/30  train=6984.8786  valid=6657.7442\n",
      "[LSTM] Epoch 3/30  train=6772.7560  valid=6530.1800\n",
      "[LSTM] Epoch 4/30  train=6697.8665  valid=6465.7722\n",
      "[LSTM] Epoch 5/30  train=6659.7450  valid=6442.7615\n",
      "[LSTM] Epoch 6/30  train=6638.3231  valid=6437.2098\n",
      "[LSTM] Epoch 7/30  train=6626.6709  valid=6418.9379\n",
      "[LSTM] Epoch 8/30  train=6605.2879  valid=6397.1863\n",
      "[LSTM] Epoch 9/30  train=6600.6172  valid=6391.6484\n",
      "[LSTM] Epoch 10/30  train=6601.0657  valid=6389.8214\n",
      "[LSTM] Epoch 11/30  train=6588.7565  valid=6377.0939\n",
      "[LSTM] Epoch 12/30  train=6573.7113  valid=6352.4582\n",
      "[LSTM] Epoch 13/30  train=6570.0298  valid=6353.9984\n",
      "[LSTM] Epoch 14/30  train=6565.5290  valid=6356.6081\n",
      "[LSTM] Epoch 15/30  train=6566.3707  valid=6345.0752\n",
      "[LSTM] Epoch 16/30  train=6559.6452  valid=6353.1143\n",
      "[LSTM] Epoch 17/30  train=6563.1800  valid=6342.7511\n",
      "[LSTM] Epoch 18/30  train=6559.2997  valid=6343.4693\n",
      "[LSTM] Epoch 19/30  train=6562.6221  valid=6342.2470\n",
      "[LSTM] Epoch 20/30  train=6552.4570  valid=6334.1592\n",
      "[LSTM] Epoch 21/30  train=6551.8566  valid=6359.6323\n",
      "[LSTM] Epoch 22/30  train=6550.9130  valid=6332.2214\n",
      "[LSTM] Epoch 23/30  train=6552.1292  valid=6330.8905\n",
      "[LSTM] Epoch 24/30  train=6551.3674  valid=6333.5833\n",
      "[LSTM] Epoch 25/30  train=6541.6501  valid=6341.1172\n",
      "[LSTM] Epoch 26/30  train=6545.8124  valid=6342.9683\n",
      "[LSTM] Epoch 27/30  train=6543.8130  valid=6328.5150\n",
      "[LSTM] Epoch 28/30  train=6540.8649  valid=6330.5242\n",
      "[LSTM] Epoch 29/30  train=6541.0114  valid=6325.7410\n",
      "[LSTM] Epoch 30/30  train=6535.4924  valid=6333.5434\n",
      "\n",
      "=== Training VAE ===\n",
      "[VAE ] Epoch 1/30  train=8437.3235  valid=7209.8460\n",
      "[VAE ] Epoch 2/30  train=7088.8338  valid=6740.6537\n",
      "[VAE ] Epoch 3/30  train=6826.7052  valid=6564.4481\n",
      "[VAE ] Epoch 4/30  train=6731.1177  valid=6552.2554\n",
      "[VAE ] Epoch 5/30  train=6683.4557  valid=6459.4912\n",
      "[VAE ] Epoch 6/30  train=6659.4717  valid=6433.4358\n",
      "[VAE ] Epoch 7/30  train=6637.0927  valid=6412.6555\n",
      "[VAE ] Epoch 8/30  train=6625.7714  valid=6428.6032\n",
      "[VAE ] Epoch 9/30  train=6618.4706  valid=6403.9560\n",
      "[VAE ] Epoch 10/30  train=6605.5176  valid=6393.5026\n",
      "[VAE ] Epoch 11/30  train=6598.7513  valid=6387.0303\n",
      "[VAE ] Epoch 12/30  train=6605.5362  valid=6411.5129\n",
      "[VAE ] Epoch 13/30  train=6596.0300  valid=6381.4283\n",
      "[VAE ] Epoch 14/30  train=6585.6083  valid=6389.7082\n",
      "[VAE ] Epoch 15/30  train=6598.5121  valid=6389.2546\n",
      "[VAE ] Epoch 16/30  train=6583.1532  valid=6374.1311\n",
      "[VAE ] Epoch 17/30  train=6576.1862  valid=6367.0507\n",
      "[VAE ] Epoch 18/30  train=6576.3532  valid=6343.8242\n",
      "[VAE ] Epoch 19/30  train=6581.3947  valid=6396.0390\n",
      "[VAE ] Epoch 20/30  train=6576.4068  valid=6376.6045\n",
      "[VAE ] Epoch 21/30  train=6574.0951  valid=6352.5509\n",
      "[VAE ] Epoch 22/30  train=6567.7079  valid=6384.7344\n",
      "[VAE ] Epoch 23/30  train=6565.1353  valid=6349.7500\n",
      "[VAE ] Epoch 24/30  train=6562.9096  valid=6344.0426\n",
      "[VAE ] Epoch 25/30  train=6558.7023  valid=6361.5073\n",
      "[VAE ] Epoch 26/30  train=6557.8111  valid=6359.4051\n",
      "[VAE ] Epoch 27/30  train=6557.4043  valid=6351.3024\n",
      "[VAE ] Epoch 28/30  train=6567.5108  valid=6363.1622\n",
      "[VAE ] Epoch 29/30  train=6552.6153  valid=6355.8210\n",
      "[VAE ] Epoch 30/30  train=6552.4111  valid=6352.4055\n",
      "\n",
      "=== Evaluation on Test ===\n",
      "\n",
      "=== Top-5 Regressed (VAE best >> LSTM) ===\n",
      "  idx     LSTM_MSE  VAE_best_MSE          delta           y_true                                  y_LSTM                               y_VAE_best  Diversity_std\n",
      "73315 1.623542e+06  1.745667e+06 -122124.875000 [1208.0, 1756.0] [272.7423095703125, 215.74765014648438]  [206.3621826171875, 178.64288330078125]       0.207360\n",
      "59864 1.216874e+06  1.316858e+06  -99983.875000  [1737.0, 599.0] [236.79470825195312, 171.0604248046875] [175.75570678710938, 156.01889038085938]       0.323004\n",
      "22156 9.078337e+04  1.875731e+05  -96789.726562      [10.0, 2.0]  [336.9127197265625, 275.3034973144531]    [431.11572265625, 446.75579833984375]       2.087747\n",
      "30354 1.706437e+05  2.454131e+05  -74769.437500   [850.0, 384.0] [284.4615478515625, 237.52935791015625]  [185.0813751220703, 163.29779052734375]       0.219815\n",
      "80204 2.276782e+04  8.898319e+04  -66215.375000   [532.0, 472.0]    [376.8749084472656, 325.46728515625]  [229.8272705078125, 177.62265014648438]       1.059305\n",
      "\n",
      "=== Win-rate by improvement bucket (Δ = LSTM MSE − VAE best MSE) ===\n",
      "Improvement bucket  count  ratio\n",
      "     VAE<<劣(>1000)   6869 0.0766\n",
      "    VAE>>大勝(>1000)   6885 0.0768\n",
      "       VAE劣(10~50)   6319 0.0705\n",
      "    VAE劣(200~1000)  10992 0.1227\n",
      "      VAE劣(50~200)   9516 0.1062\n",
      "      VAE勝(50~200)  18166 0.2027\n",
      "   VAE大勝(200~1000)  16126 0.1799\n",
      "        VAE略劣(<10)   2587 0.0289\n",
      "      VAE略勝(10~50)   9207 0.1027\n",
      "                打平   2951 0.0329\n",
      "\n",
      "========== 評估結果（原始尺度） ========== \n",
      "LSTM  MSE (整體): 6613.6284\n",
      "VAE   Best-of-N MSE: 6592.6974  (N=40)\n",
      "VAE   Diversity (std): 0.1623\n",
      "VAE   Coverage (比例，使用勝率近似): 0.5951\n",
      "門檻 tau = LSTM 每序列 MSE 中位數 = 835.3912\n",
      "\n",
      "Saved models to outputs\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    # 固定參數設定（取代 argparse）\n",
    "    data_dir   = '.'                # OULAD CSV 資料夾路徑\n",
    "    out_dir    = './outputs'        # 模型輸出目錄\n",
    "    epochs     = 30                 # 訓練輪數\n",
    "    batch_size = 128                # 每批次資料量\n",
    "    hidden_dim = 64                 # LSTM 隱藏層維度\n",
    "    latent_dim = 16                 # VAE 潛在維度\n",
    "    beta       = 0.01               # VAE KLD loss 權重\n",
    "    samples    = 40                 # VAE Best-of-N samples 數\n",
    "    seed       = 42                 # 隨機種子\n",
    "\n",
    "    # 設定環境\n",
    "    set_seed(seed)\n",
    "    out_dir = Path(out_dir)\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    # 載入資料\n",
    "    print(\"Loading OULAD CSVs...\")\n",
    "    info, vle, asm = load_oulad(data_dir)\n",
    "\n",
    "    # 建立週序列資料\n",
    "    print(\"Building weekly sequences...\")\n",
    "    weekly, sample_data = build_weekly_sequences(info, vle, asm, in_weeks=4, out_weeks=2)\n",
    "    print(f\"Weekly rows: {len(weekly):,}   Samples: {len(sample_data):,}\")\n",
    "\n",
    "    # 依 student_id 分割\n",
    "    train_df, valid_df, test_df = split_by_student(sample_data, seed=seed)\n",
    "    print(f\"Train samples: {len(train_df):,} | Valid: {len(valid_df):,} | Test: {len(test_df):,}\")\n",
    "\n",
    "    # 建立資料張量\n",
    "    X_tr, y_tr = train_df['X'].to_list(), train_df['y'].to_list()\n",
    "    X_va, y_va = valid_df['X'].to_list(), valid_df['y'].to_list()\n",
    "    X_te, y_te = test_df['X'].to_list(),  test_df['y'].to_list()\n",
    "\n",
    "    tr_loader = DataLoader(SeqDataset(X_tr, y_tr), batch_size=batch_size, shuffle=True)\n",
    "    va_loader = DataLoader(SeqDataset(X_va, y_va), batch_size=batch_size, shuffle=False)\n",
    "    te_loader = DataLoader(SeqDataset(X_te, y_te), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 選擇裝置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # 建立模型\n",
    "    lstm = Seq2SeqLSTM(in_dim=4, hidden_dim=hidden_dim, out_steps=2)\n",
    "    vae  = Seq2SeqVAE(in_dim=4, hidden_dim=hidden_dim, latent_dim=latent_dim, out_steps=2)\n",
    "\n",
    "    # 訓練\n",
    "    print(\"\\n=== Training LSTM ===\")\n",
    "    lstm = train_lstm(lstm, tr_loader, va_loader, epochs, 1e-3, device)\n",
    "    print(\"\\n=== Training VAE ===\")\n",
    "    vae  = train_vae(vae, tr_loader, va_loader, epochs, 1e-3, beta, device)\n",
    "\n",
    "    # 評估\n",
    "    print(\"\\n=== Evaluation on Test ===\")\n",
    "    df, buckets, results = evaluate(\n",
    "        (lstm, vae),\n",
    "        {'test': te_loader},\n",
    "        device,\n",
    "        samples=samples,\n",
    "        out_dir=out_dir\n",
    "    )\n",
    "\n",
    "    # 儲存模型\n",
    "    torch.save(lstm.state_dict(), out_dir / \"lstm.pt\")\n",
    "    torch.save(vae.state_dict(),  out_dir / \"vae.pt\")\n",
    "    print(f\"\\nSaved models to {out_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
